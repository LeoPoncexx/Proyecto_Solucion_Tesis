from flask import Flask, request, jsonify, render_template
from llama_cpp import Llama
import time

app = Flask(__name__)

# Ruta al modelo GGUF
modelo_path = "C:/Users/hlpc0/Desktop/tesis/Page_Tesis/openhermes.gguf"

# NÃºmero de capas en GPU (ajustado para GTX 1650)
gpu_layers = 32

# Cargar modelo
print("\nðŸ§  Cargando modelo...")
llm = Llama(
    model_path=modelo_path,
    n_ctx=1024,
    n_threads=6,
    n_batch=64,
    n_gpu_layers=gpu_layers
)
print("âœ… Modelo cargado.\n")

if gpu_layers > 0:
    print(f"ðŸ” GPU activada con {gpu_layers} capas (si tu instalaciÃ³n tiene cuBLAS).")
else:
    print("âš ï¸ Ejecutando completamente en CPU.")

# Prompt del sistema optimizado
system_prompt = (
    "Eres un asistente experto en educaciÃ³n financiera en PerÃº. "
    "Responde solo en espaÃ±ol, de forma clara, breve y con frases separadas por saltos de lÃ­nea. "
    "Evita explicaciones largas. Resume en mÃ¡ximo 2 frases.\n"
)
historial = system_prompt

# Respuestas predefinidas
respuestas_directas = {
    "hola": "Â¡Hola! Â¿En quÃ© puedo ayudarte sobre educaciÃ³n financiera?",
    "buenas": "Â¡Buenas! Â¿Quieres saber sobre ahorro, AFP, presupuesto u otro tema?",
    "como estas": "Estoy lista para ayudarte con tu educaciÃ³n financiera. ðŸ˜Š",
    "de que trata esta pagina": "Esta pÃ¡gina estÃ¡ diseÃ±ada para ayudarte a mejorar tu educaciÃ³n financiera con ayuda de una IA.",
    "quien te creo": "Fui creada por Hiver Leonardo Ponce Contreras y Niwell Gonzalo Quispe LingÃ¡n."
}

@app.route("/")
def index():
    return render_template("index.html")

@app.route("/chat", methods=["POST"])
def chat():
    global historial
    user_input = request.json.get("message", "").lower().strip()

    # Verificar respuestas cortas predefinidas
    for frase, respuesta in respuestas_directas.items():
        if frase in user_input:
            return jsonify({"reply": respuesta, "tiempo": 0})

    # Preparar prompt
    prompt = historial + f"Usuario: {user_input}\nAsistente:"
    inicio = time.time()

    # Generar respuesta breve
    respuesta = llm(
        prompt,
        max_tokens=80,  # â±ï¸ Respuestas cortas y rÃ¡pidas
        temperature=0.2,
        top_p=0.9,
        stop=["Usuario:", "Asistente:"]
    )

    tiempo = round(time.time() - inicio, 2)
    texto = respuesta["choices"][0]["text"].strip()

    # Truncar si se pasa del lÃ­mite visual
    if len(texto) > 350:
        texto = texto[:350].rsplit(".", 1)[0] + "."

    # AÃ±adir al historial
    historial += f"Usuario: {user_input}\nAsistente: {texto}\n"

    # Limpiar historial si es muy largo
    if historial.count("Usuario:") > 15:
        historial = system_prompt

    return jsonify({"reply": texto, "tiempo": tiempo})

if __name__ == "__main__":
    app.run(debug=True)
